---
title: 'Course Recap: Predictive Modelling'
date: 2025-01-08
permalink: /posts/2025/01/predmod/
tags:
  - course-recap
  - cogsci-masters
  - '2025'
---

#### **Course Overview:**
This course provided a comprehensive exploration of predictive modelling techniques, focusing on their statistical foundations and practical applications. With a strong emphasis on data analysis—including statistics, machine learning, dynamic modelling, and filtering—the course highlighted their relevance for cognitive neuroscience. Hands-on Python exercises enriched the learning experience, solidifying my ability to apply these techniques to real-world datasets.

#### Key Concepts and Applications:

1. **Dimensionality Reduction**  
   Methods like PCA and Factor Analysis were invaluable for simplifying high-dimensional data, such as brain imaging or electrophysiological recordings. These techniques uncover patterns and reduce noise, streamlining data analysis in neuroscience.

2. **Classification**  
   Techniques like k-nearest neighbours, SVM, and neural networks ([**check Here**](_posts\2025-01-08-course-recap-deep-learning.md) for reflection on separate deep learning course I took) were introduced for categorizing and predicting brain states or behaviours. These tools can decode brain activity and classify cognitive or behavioural outcomes.

3. **Clustering**  
   Methods like k-means, DBSCAN, and GMM allowed for grouping similar data points without prior labels. Applications include identifying functional brain networks, segmenting brain regions, or grouping neurons with shared response properties.

4. **Time Series Analysis**  
   Stationary time series, autocorrelation, and forecasting models (e.g., AR, MA, SARIMA) were covered, essential for analyzing time-varying neural signals and understanding temporal dependencies in brain activity.

5. **Dynamic Linear Models (DLM) and Kalman Filter**  
   These tools provided frameworks for tracking and estimating the evolution of systems over time, applicable to decoding dynamic brain processes or tracking neural trajectories.

6. **Markov Chain Monte Carlo (MCMC) Methods**  
   The course highlighted MCMC as a powerful tool for sampling from complex probability distributions. The **Metropolis-Hastings algorithm**, in particular, stood out for its ability to:
   - Model intricate processes by constructing Markov chains that approximate desired distributions.
   - Estimate parameters in Bayesian models, incorporating prior knowledge and uncertainty into analyses which is very relevant in computation science.
   - Facilitate hypothesis testing and model comparison using posterior distributions and metrics like Bayes factors.

#### Reflection:

This course underscored the statistical and computational underpinnings of predictive modelling, offering a me robust toolkit for understanding brain function through data-driven approaches. The emphasis on Python exercises allowed me to directly apply these techniques to data that are not neurosience but these experiences are highgly transferable, bridging theoretical concepts with practical implementation. While I gained significant insights, I aim to deepen my understanding of advanced topics like MCMC to enhance their applications in cognitive neuroscience. Overall, I love this course and would.

I will hope to post some of the cool projects I did in this course in my portfolio page. 

[Course link: Predictive Modelling](https://www.hse.ru/en/edu/courses/922873288)